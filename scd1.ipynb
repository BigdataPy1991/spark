{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/FileStore/tables/hist_data_csv-72c06.txt\n",
    "#/FileStore/tables/incr_data.csv\n",
    "from pyspark.sql.types import *\n",
    "hist_df=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/tables/hist_data_csv-72c06.txt\")\n",
    "incr_df=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/tables/incr_data.csv\")\n",
    "\n",
    "#hist_df.show()\n",
    "#incr_df.show()\n",
    "#hist_df.printSchema()\n",
    "#incr_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+----+--------+----------+----------+---------+\n",
       " Id|Name|   Place| Eff_start|   Eff_end|Indicator|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "  2|Hari|     Hyd|2019-11-15|9999-12-31|        1|\n",
       "  3|Nick|Banglore|2019-11-15|9999-12-31|        1|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "\n",
       "+---+----+-----+----------+----------+---------+\n",
       " Id|Name|Place| Eff_start|   Eff_end|Indicator|\n",
       "+---+----+-----+----------+----------+---------+\n",
       "  6|John|  Hyd|2019-11-16|9999-12-31|        1|\n",
       "+---+----+-----+----------+----------+---------+\n",
       "\n",
       "+---+----+--------+----------+----------+---------+\n",
       " Id|Name|   Place| Eff_start|   Eff_end|Indicator|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "  1|Liji|Banglore|2019-10-10|9999-12-31|        1|\n",
       "  4| Ram| Chennai|2019-10-13|9999-12-31|        1|\n",
       "  5| Sam|     Hyd|2019-10-14|9999-12-31|        1|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "\n",
       "+---+----+--------+----------+----------+---------+\n",
       " Id|Name|   Place| Eff_start|   Eff_end|Indicator|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "  2|Hari| Chennai|2019-10-11|2020-04-24|        0|\n",
       "  3|Nick|Banglore|2019-10-12|2020-04-24|        0|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "\n",
       "+---+----+--------+----------+----------+---------+\n",
       " Id|Name|   Place| Eff_start|   Eff_end|Indicator|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "  1|Liji|Banglore|2019-10-10|9999-12-31|        1|\n",
       "  2|Hari| Chennai|2019-10-11|2020-04-24|        0|\n",
       "  2|Hari|     Hyd|2019-11-15|9999-12-31|        1|\n",
       "  3|Nick|Banglore|2019-11-15|9999-12-31|        1|\n",
       "  3|Nick|Banglore|2019-10-12|2020-04-24|        0|\n",
       "  4| Ram| Chennai|2019-10-13|9999-12-31|        1|\n",
       "  5| Sam|     Hyd|2019-10-14|9999-12-31|        1|\n",
       "  6|John|     Hyd|2019-11-16|9999-12-31|        1|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "df_inter=incr_df.alias(\"t1\").join(hist_df.alias(\"t2\"),col(\"t1.Id\") == col(\"t2.Id\"),'inner').select(col(\"t1.Id\"),col(\"t1.Name\"),col(\"t1.Place\"),col(\"t1.Eff_start\"),col(\"t2.Eff_end\"),col(\"t2.Indicator\"))\n",
    "df_inter.show()\n",
    "\n",
    "df_new=incr_df.alias(\"t1\").join(hist_df.alias(\"t2\"),col(\"t1.Id\") ==col(\"t2.Id\"),'left').filter(col(\"t2.Id\").isNull()).select(col(\"t1.Id\"),col(\"t1.Name\"),col(\"t1.Place\"),col(\"t1.Eff_start\"),col(\"t2.Eff_end\"),col(\"t2.Indicator\")).withColumn(\"Eff_end\",lit(\"9999-12-31\")).withColumn(\"Indicator\",lit(\"1\"))\n",
    "\n",
    "df_new.show()\n",
    "\n",
    "df_old = hist_df.alias(\"t1\").join(incr_df.alias(\"t2\"),col(\"t1.Id\") == col(\"t2.Id\"),'left').select(\"t1.*\").filter(col(\"t2.Id\").isNull())\n",
    "df_old.show()\n",
    "\n",
    "df_old_exp = incr_df.alias(\"t1\").join(hist_df.alias(\"t2\"),col(\"t1.Id\") == col(\"t2.Id\"),'inner').select(\"t2.*\").withColumn(\"Eff_end\",current_date()).withColumn(\"Indicator\",lit(\"0\"))\n",
    "df_old_exp.show()\n",
    "\n",
    "\n",
    "df_final=df_new.unionAll(df_inter).unionAll(df_old_exp).unionAll(df_old)\n",
    "df_final.orderBy(\"Id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+----+--------+----------+----------+---------+\n",
       " Id|Name|   Place| Eff_start|   Eff_End|Indicator|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "  2|Hari|     Hyd|2019-11-15|9999-12-31|        1|\n",
       "  3|Nick|Banglore|2019-11-15|9999-12-31|        1|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "\n",
       "+---+----+--------+----------+----------+---------+\n",
       " Id|Name|   Place| Eff_start|   Eff_end|Indicator|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "  1|Liji|Banglore|2019-10-10|9999-12-31|        1|\n",
       "  4| Ram| Chennai|2019-10-13|9999-12-31|        1|\n",
       "  5| Sam|     Hyd|2019-10-14|9999-12-31|        1|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "\n",
       "+---+----+-----+----------+----------+---------+\n",
       " Id|Name|Place| Eff_start|   Eff_end|Indicator|\n",
       "+---+----+-----+----------+----------+---------+\n",
       "  6|John|  Hyd|2019-11-16|9999-12-31|        1|\n",
       "+---+----+-----+----------+----------+---------+\n",
       "\n",
       "+---+----+--------+----------+----------+---------+\n",
       " Id|Name|   Place| Eff_start|   Eff_end|Indicator|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "  6|John|     Hyd|2019-11-16|9999-12-31|        1|\n",
       "  1|Liji|Banglore|2019-10-10|9999-12-31|        1|\n",
       "  4| Ram| Chennai|2019-10-13|9999-12-31|        1|\n",
       "  5| Sam|     Hyd|2019-10-14|9999-12-31|        1|\n",
       "  2|Hari|     Hyd|2019-11-15|9999-12-31|        1|\n",
       "  3|Nick|Banglore|2019-11-15|9999-12-31|        1|\n",
       "+---+----+--------+----------+----------+---------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "df_newrec = incr_df.alias(\"t1\").join(hist_df.alias(\"t2\"),col(\"t1.Id\") == col(\"t2.Id\"),\"left\").filter(col(\"t2.Id\").isNull()).select(\"t1.*\",col(\"t2.Eff_end\"),col(\"t2.Indicator\")).withColumn(\"Eff_end\",lit(\"9999-12-31\")).withColumn(\"Indicator\",lit(\"1\"))\n",
    "#.withCo)lumn(\"Eff_end\",\"9999-12-31\").withColumn(\"Indicator\",\"1\")\n",
    "\n",
    "df_oldrec = incr_df.alias(\"t1\").join(hist_df.alias(\"t2\"),col(\"t1.Id\") == col(\"t2.Id\"),\"right\").filter(col(\"t1.Id\").isNull()).select(\"t2.*\")\n",
    "df_uprec=incr_df.alias(\"t1\").join(hist_df.alias(\"t2\"),col(\"t1.Id\")==col(\"t2.Id\"),\"inner\").select(\"t1.*\",col(\"t2.Eff_end\"),col(\"t2.Indicator\")).withColumn(\"Eff_End\",lit(\"9999-12-31\")).withColumn(\"Indicator\",lit(\"1\"))\n",
    "df_uprec.show()\n",
    "#.withColumn(\"Eff_end\",lit(\"9999-12-31\")).withColumn(\"Indicator\",lit(\"1\"))\n",
    "df_oldrec.show()\n",
    "df_newrec.show()\n",
    "\n",
    "dffinal=df_newrec.unionAll(df_oldrec).unionAll(df_uprec)\n",
    "dffinal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "scd1",
  "notebookId": 3090387114134363
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
